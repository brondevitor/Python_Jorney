{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação\n",
    "\n",
    "Entraremos num capítulo que trata de um dos problemas mais comuns para Machine Learning: classificação! Em problemas de classificação, temos um dataset com elementos de diversas classes e temos que ser capazes de treinar o algorítmo para separar estas classes. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "# O que é Classificação?\n",
    "\n",
    "Classificação é o processo de prever a classe de determinados pontos de dados. Às vezes, as classes são chamadas de destinos/marcadores ou categorias. A modelagem preditiva de classificação é a tarefa de aproximar uma função de mapeamento (f) de variáveis de entrada (X) para variáveis de saída discretas (y).\n",
    "\n",
    "A classificação pertence à categoria de aprendizado supervisionado, onde os objetivos também são fornecidos com os dados de entrada, e é melhor usado quando a saída possui valores finitos e discretos. \n",
    "\n",
    "Existem 2 tipos de classificação:\n",
    "\n",
    "- Binomial\n",
    "- Multi-Class\n",
    "\n",
    "\n",
    "Existem muitas aplicações na classificação em muitos domínios, como:\n",
    "\n",
    "- Aprovação de crédito\n",
    "- Diagnóstico médico\n",
    "- Marketing direcionado\n",
    "\n",
    "Outros exemplos mais específicos de problemas de classificação:\n",
    "\n",
    "- Descobrir se um email recebido é spam \n",
    "- Inferir se um empréstimo bancário deve ser concedido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de algoritmos de classificação\n",
    "\n",
    "Vamos dar uma olhada rápida nos tipos de algoritmo de classificação abaixo, segundo a estratégia de abordagem para o cálculo dos otimizadores das funções (mappings):\n",
    "\n",
    "1. Aprendizado por minimização dos erros (error-based learning)\n",
    "    - Regressão Logística\n",
    "    - Suport Vector Machines (SVM)\n",
    "\n",
    "\n",
    "2. Aprendizado por similaridade (similarity-based learning)\n",
    "    - K-Nearest Neighbors (KNN)\n",
    "\n",
    "\n",
    "3. Aprendizado por probabilidades (probability-based learning)\n",
    "    - Naive Bayes\n",
    "\n",
    "\n",
    "4. Aprendizado por ganho de informação (information-based learning)\n",
    "    - Árvores de Decisão (Decision Trees)\n",
    "    - Random Forest \n",
    "    \n",
    "Nesta aula, iremos ampliar os conceitos de Regressão Logística, e estudar os algoritmos SVM e KNN. Nas próximas aulas, nos aprofundaremos em Naive Bayes e Decision Trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "# K-Nearest Neighbors (KNN)\n",
    "\n",
    "O algoritmo KNN pressupõe que coisas semelhantes existem próximas. Em outras palavras, coisas semelhantes estão próximas umas das outras.\n",
    "\n",
    "\"Pássaros de uma mesma espécie voam juntos.\"<br>\n",
    "\"Diga-me com quem andas e eu direi quem és.\"\n",
    "\n",
    "<br>\n",
    "<img src=\"knn.png\" align=\"center\" width=350>\n",
    "<br>\n",
    "\n",
    "Observe na imagem acima que, na maioria das vezes, pontos de dados semelhantes estão próximos um do outro. O algoritmo KNN depende dessa suposição, sendo verdadeira o suficiente para que o algoritmo seja útil. O KNN captura a idéia de similaridade (às vezes chamada de distância, proximidade ou proximidade) com algumas matemáticas que aprendemos na infância - calculando a distância entre os pontos de um gráfico.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Euclidean_distance_2d.svg/1200px-Euclidean_distance_2d.svg.png\" align='center' width=350>\n",
    "<br>\n",
    "\n",
    "\n",
    "Nota: É necessário entender como calculamos a distância entre os pontos em um gráfico antes de prosseguir. Existem diversas maneiras de calcular a distância, e uma delas pode ser preferível, dependendo do problema que estamos resolvendo. No entanto, a distância em linha reta (também chamada de distância euclidiana) é uma escolha popular e familiar.\n",
    "\n",
    "<img src=\"knn.gif\" align=\"center\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mais detalhes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://arxiv.org/pdf/1708.04321.pdf'>  Artigo sobre Distâncias <a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prós:\n",
    "- Simples implementação\n",
    "- Poucos parâmetros (distância e K)\n",
    "- Lida com problemas binominais e multi classe\n",
    "- Trata bem problemas não lineares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contras:\n",
    "- Alto custo computacional (calculo das distâncias ponto a ponto)\n",
    "- É um lazzy learner\n",
    "- Não lida tão bem com alta dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# SVM (Suport Vector Machines)\n",
    "\n",
    "O objetivo do algoritmo da SVM é encontrar um hiperplano N-1 dimensional em um espaço N-dimensional (N - o número de recursos) que classifica distintamente os pontos de dados. O SVM é um Maximal Margin Classifier\n",
    "\n",
    "<br>\n",
    "<img src=\"svm.png\" align=\"center\" width=500>\n",
    "<br>\n",
    "\n",
    "Para separar as duas classes de pontos de dados, existem muitos hiperplanos possíveis que podem ser escolhidos. Nosso objetivo é encontrar um plano com a margem máxima, ou seja, a distância máxima entre os pontos de dados das duas classes. A maximização da distância da margem fornece algum reforço para que os pontos de dados futuros possam ser classificados com mais confiança.\n",
    "\n",
    "\n",
    "## Hiperplanos e vetores de suporte\n",
    "\n",
    "<br>\n",
    "<img src=\"svm_hyperplane.png\" align=\"center\" width=500>\n",
    "<br>\n",
    "\n",
    "Hiperplanos são limites de decisão que ajudam a classificar os pontos de dados. Os pontos de dados que caem em ambos os lados do hiperplano podem ser atribuídos a diferentes classes. Além disso, a dimensão do hiperplano depende do número de recursos. Se o número de recursos de entrada for 2, o hiperplano será apenas uma linha. Se o número de recursos de entrada for 3, o hiperplano se tornará um plano bidimensional. Torna-se difícil imaginar quando o número de recursos excede 3.\n",
    "\n",
    "Os vetores de suporte são pontos de dados que estão mais próximos do hiperplano e influenciam a posição e a orientação do hiperplano. Usando esses vetores de suporte, maximizamos a margem do classificador. A exclusão dos vetores de suporte alterará a posição do hiperplano. Esses são os pontos que nos ajudam a criar nosso SVM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuição de margem grande\n",
    "\n",
    "Na regressão logística, pegamos a saída da função linear e comprimimos o valor dentro da faixa de [0,1] usando a função sigmóide. Se o valor compactado for maior que um valor limite (0,5), atribuímos a ele um rótulo 1, caso contrário, atribuímos a ele um rótulo 0. \n",
    "\n",
    "No SVM, obtemos a saída da função linear e, se essa saída for maior que 1, identificamos com uma classe e se a saída for -1, identificamos com outra classe. Como os valores limite são alterados para 1 e -1 no SVM, obtemos esse intervalo de valores de reforço ([- 1,1]) que atua como margem.\n",
    "\n",
    "Hiperplanos com margens maiores apresentam menor erro de generalização. Os hiperplanos positivos e negativos são representados por:\n",
    "\n",
    "$ w_0 + w^T x_{pos} = 1 $ \\\n",
    "$ w_0 + w^T x_{pos} = -1 $ \n",
    "\n",
    "Portanto, o objetivo da função é maximizar com a restrição de que as amostras sejam classificadas corretamente, o que é representado como:\n",
    "\n",
    "\n",
    "$ w_0 + w^T x^{(i)} \\geqslant 1 \\space  if  \\space  y^{(i)} = 1 $ \\\n",
    "$ w_0 + w^T x^{(i)} < 1 \\space  if  \\space  y^{(i)} = -1 $\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png\" align='center' width=300>\n",
    "\n",
    "### Visualização do Hiperplano de Separação\n",
    "\n",
    "<img src=\"svm.gif\" align=\"center\" width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margem Rígida (Hard Margin)\n",
    "\n",
    "Estamos falando aqui de um conjunto de dados linearmente separável, ou seja, para o qual conseguimos criar uma superfície plana e então segmentar as classes envolvidas no problema. Nesse cenário, temos um modelo de margem rígida, o qual exige que todas as amostras fiquem foram da região da margem. Qualquer mudança nos pontos próximos à margem pode provocar alteração no hiperplano. Além disso, qualquer ruído, inviabiliza a utilização do modelo!\n",
    "\n",
    "A essa altura, vocês já devem saber que o mundo não é perfeito e nem sempre temos classes linearmente separáveis. Nesse caso, introduzimos o conceito de Margem Flexível (Soft margin).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/552/1*CD08yESKvYgyM7pJhCnQeQ.png\" align='center' width=700>\n",
    "\n",
    "SVMs de Margem Flexível permitem violações ao hiperplano! Desas forma, o algoritmo fica menos suscetível a ruído e pode generalizar melhor. Adicionamos à otimização do algoritmo um parâmetro de penalização para os pontos que estão violando a condição da margem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SVM pode permitir que vários pontos violem a margem, mas ao preço de que a soma das penalizações não ultrapasse um valor C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truque do Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar da correção feita com as soft margins, a maioria dos problemas que encaramos são não-lineares e para isso é necessário introduzir um componente que faz toda diferença na construção da SVM: O Truque do Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O truque do kernel nada mais é do que uma transformação algébrica que aumenta a dimensionalidade do problema, na tentativa de torná-lo um problema linearmente separável!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1556/1*Wp8tGecatxHqUgHNaVQddg.png\" align='center' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima temos a imagem de um conjunto de pontos não linearmente separáveis. Ao usar o truque do kernel, podemos transformar esses dados, de forma que eles possam ganhar uma dimensão a mais e, nessa dimensão, eles serão linearmente separáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://gregorygundersen.com/image/kerneltrick/idea.png\" align=\"center\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/1400/1*gIHnZCcl4Q9fFx2AZsJ7pw.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a transformação podemos construir um hiperplano de separação!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basicamente, existem 3 tipos de transformações:\n",
    "- Linear\n",
    "- Gaussiana (RBF - Base Radial)\n",
    "- Polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prós:\n",
    "- Lida bem com alta dimensionalidade\n",
    "- Matemática robusta e bem definida (existência de prova matemática)\n",
    "\n",
    "Contras:\n",
    "- Lento com datasets de muitas linhas\n",
    "- Não produz probabilidades de forma nativa (tem como fazer gambiarra)\n",
    "- Não lida de forma nativa com problemas multi classe (tem como fazer gambiarra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "# Métricas: Matriz de Confusão\n",
    "\n",
    "Numa classificação binária, temos 4 possíveis *outputs*: True Positive, False Positive, False Negative e True Negative. Podemos visualizar esses outputs numa matriz de confusão como essa: \n",
    "\n",
    "<img src=\"confusion_matrix.png\" align=\"center\" width=\"60%\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Como podemos observar acima, trabalhamos com 4 métricas sobre essa matriz de confusão. Elas são:\n",
    " - **Accuracy**: de tudo o que você classificou, qual parte você acertou. É a primeira métrica que olhamos, mas algumas vezes ela pode não responder nossas perguntas de modo satisfatório e ocultar o que está acontecendo com os erros.\n",
    " - **Precision**: fração dos dados categorizados positivamente que são, de fato, casos positivos. É útil para sabermos quão confiável é nossa previsão para positivo.\n",
    " - **Recall / Sensivity**: fração de dados positivos categorizados de fato como positivos. Mostra como nosso modelo enxerga os dados positivos\n",
    " - **Specificity**: fração de dados negativos categorizados de fato como negativos. Mostra como nosso modelo enxerga os dados negativos.\n",
    "\n",
    "<br>\n",
    "Para cada problema de classificação binária, é interessante utilizar uma ou mais dessas métricas. A ideia é que sempre tenhamos controle não só da nossa acurácia, mas como estamos acertando e errando com nosso modelo, de modo a entender como melhorá-lo e quais as consequências dele nas previsões. Além das 4 métricas acima, também utilizamos o F1 Score, que é um balanço entre Precision e Recall, calculado por F1 = 2*((precision * recall) / (precision + recall)). <br>\n",
    "\n",
    "\n",
    "Com essas métricas em mãos, nos levamos à uma questão mais primordial: como escolher uma métrica para meu problema? Embora não exista uma resposta pronta para essa pergunta, é sempre interessante observarmos acurácia, precision e recall, juntas, para termos um entendimento do que está acontecendo :) <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curva ROC e métrica AUC (Area Under the Curve)\n",
    "\n",
    "A curva ROC e a métrica AUC é uma medida de desempenho para o problema de classificação em várias configurações de limites. ROC é uma curva de probabilidade e AUC representa grau ou medida de separabilidade. Diz quanto modelo é capaz de distinguir entre classes. Quanto maior a AUC, melhor o modelo em prever 0s como 0s e 1s como 1s. Por analogia, quanto maior a AUC, melhor o modelo é distinguir entre pacientes com doença e sem doença.\n",
    "\n",
    "<br>\n",
    "<img src=\"roc_curve.png\" align=\"center\" width=\"350\">\n",
    "<br>\n",
    "\n",
    "A curva ROC é plotada com TPR (True Positive Rate) contra o FPR (False Positive Rate), onde TPR está no eixo y e FPR está no eixo x.\n",
    "\n",
    "<br>\n",
    "<img src=\"roc_formulas.png\" align=\"center\" width=\"350\">\n",
    "<br>\n",
    "\n",
    "Separei alguns links para vocês irem mais a fundo nesse assunto de interpretar e escolher métricas:\n",
    "\n",
    " - __[Classification Accuracy is Not Enough: More Performance Measures You Can Use](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)__\n",
    " - __[Data Science Performance Metrics for Everyone](https://towardsdatascience.com/data-science-performance-metrics-for-everyone-4d68f4859eef)__\n",
    " - __[Measuring Model Goodness — Part 1](https://towardsdatascience.com/measuring-model-goodness-part-1-a24ed4d62f71)__\n",
    " - __[The 3 Pillars of Binary Classification: Accuracy, Precision & Recall](https://medium.com/@yashwant140393/the-3-pillars-of-binary-classification-accuracy-precision-recall-d2da3d09f664)__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
