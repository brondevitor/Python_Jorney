{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/dh_logo.png\" align=\"right\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Fala galera! Tudo bem? Nas próximas duas aulas, nosso foco será em um dos problemas mais tradicionais de machine learning: clustering! Muitas vezes, ao adquirir nossos dados, precisamos deles classificados para realizar nossas análises. Porém, é possível que os dados que você esteja tratando não carreguem labels de agrupamento :( Nesse caso, o que fazemos? É claro que existe a solução trivial de deixar um estagiário fazendo esse trabalho de labelling (brincadeira, não façam isso haha). Para esse problema, podemos utilizar algoritmos de clustering que, de modo não-supervisionado, conseguem identificar padrões no nosso dataset e separá-lo em grupos.\n",
    "\n",
    "\n",
    "A aprendizagem supervisionada consiste em inferir uma função a partir de dados de treinamento rotulados que eventualmente serão usados ​​para mapear um novo conjunto de dados. Ok, esta é mais ou menos a definição padrão, mas deixe-me colocar em palavras simples. Digamos que temos um conjunto de variáveis ​​de entrada (X) e variáveis ​​de saída (Y). Podemos usar um algoritmo para inferir a função f que mapeia esses dados, a saber, Y = f (X). Este conjunto de dados é denominado conjunto de dados de treinamento. Uma vez que o algoritmo estimou a função de mapeamento com muita precisão após muitas iterações, podemos aplicá-lo a um novo conjunto de dados de entrada X 'para inferir Y' para esse conjunto específico. Classificação e regressão são dois tipos de problemas de aprendizagem supervisionada.\n",
    "\n",
    "Aprendizagem não supervisionada é inferir uma função para descrever estruturas ocultas de dados não rotulados (definição padrão, novamente). Em palavras simples: digamos que temos apenas dados de entrada X e nenhuma saída correspondente, ou seja, não temos um modelo para aplicar ao nosso conjunto de dados. Como podemos aprender mais sobre os dados? Devemos modelar a estrutura ou distribuição subjacente dos dados ... uma maneira eficiente de fazer isso é usar algoritmos de agrupamento.\n",
    "_______\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "Cada algoritmo de agrupamento vem em duas variantes: uma classe, que implementa o método __fit__ de ajuste para aprender os agrupamentos em dados de treinamento e uma função que, dados dados de treinamento, retorna uma matriz de rótulos de inteiros correspondentes aos diferentes agrupamentos. Para a turma, os rótulos nos dados de treinamento podem ser encontrados no atributo labels_.\n",
    "\n",
    "## Métodos de Clustering\n",
    "\n",
    "<img src=\"imgs/clustering.png\" align=\"center\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem vários métodos de clustering. No nosso curso, veremos 3 algoritmos. É necessário ter em mente que cada um deles possui limitações. No nosso curso, vamos ver três deles: KMeans, DBSCAN e HCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterizando Dados Artificiais\n",
    "\n",
    "Nessa aula, vamos começar com um exemplo mais didático e com dados gerados randomicamente pela função `make_blobs`. A ideia será gerar dados aritificiais em 3 dimensões e aplicar o K-Means para clusterizá-los. Vamos começar importando 2 módulos para plotagem e geração dos dados artificiais e preparando o nosso plot.\n",
    "\n",
    "<img src=\"imgs/make_blobs.png\" align=\"center\" width=\"65%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "#  K-Means\n",
    "\n",
    "Precisamos, agora, utilizar algum método para realizar a clusterização de nossos dados. Um dos algoritmos mais simples para isso é o K-Means. Ele consiste dos seguintes passos:\n",
    " - Iniciar k centroides randômicos no nosso dataset. No caso, k é um hiperparâmetro e indica quantos clusters estamos procurando\n",
    " - Calcular a distância de cada ponto em relação a cada um desses centroides. Assimilar o ponto ao cluster cujo centroide estiver mais próximo\n",
    " - Depois que todos os pontos estiverem assimilados a algum cluster, calcular o ponto médio de cada um desses k clusters (daí o nome, k-means). Esse ponto médio será o novo centroide.\n",
    " - Repetir o processo até que a variância se estabilize\n",
    " \n",
    " [documentação oficial do KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo o funcionamento do K-Means\n",
    "\n",
    "O K-Means é um dos algoritmos de clustering não supervisionados mais simples que é usado para agrupar nossos dados no número K de clusters. O algoritmo designa iterativamente os pontos de dados para um dos clusters K com base em quão próximo o ponto está do centróide do cluster. O resultado do algoritmo K-Means é:\n",
    "\n",
    "1. Número K de centróides do cluster\n",
    "\n",
    "\n",
    "2. Pontos de dados classificados nos clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/kmeans.gif\" align=\"center\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode conferir um tutorial de como o K-Means funciona __[nesse vídeo do StatQuest](https://www.youtube.com/watch?v=4b5d3muPQmA)__, __[nesse vídeo do Siraj](https://www.youtube.com/watch?v=9991JlKnFmk)__ ou __[nesses slides da Universidade de NY](https://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture14.pdf)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de Aplicações\n",
    "\n",
    "O K-Means pode ser usado para qualquer tipo de agrupamento em que os dados não tenham sido explicitamente rotulados. Alguns dos exemplos do mundo real são dados abaixo:\n",
    "\n",
    "- [Image Segmentation](https://www.sciencedirect.com/science/article/pii/S1877050915014143)\n",
    "- [Chromosome segmentation](https://www.ijser.org/paper/Chromosome-Segmentation-Using-K-Means-Clustering.html)\n",
    "- [News Comments Clustering](https://ieeexplore.ieee.org/document/5591774)\n",
    "- [Grouping inventory by sales activity](https://www.sciencedirect.com/science/article/pii/S1877050915035929)\n",
    "- [Clustering animals](https://www.sciencedirect.com/science/article/pii/S1877050914013155)\n",
    "- [Bots and Anomaly Detection](https://www.computerweekly.com/tip/Botnet-detection-through-DNS-behavior-and-clustering-analysis)\n",
    "\n",
    "## Esboço do Algoritmo\n",
    "\n",
    "Supondo que temos os pontos de dados de entrada x1, x2, x3,…, xn e o valor de K (o número de clusters necessários). Seguimos o procedimento abaixo:\n",
    "\n",
    "1. Escolha K pontos como os centróides iniciais do conjunto de dados, aleatoriamente ou o primeiro K.\n",
    "\n",
    "\n",
    "2. Encontre a distância euclidiana de cada ponto no conjunto de dados com os pontos K identificados (centróides de cluster)\n",
    "\n",
    "\n",
    "3. Atribua cada ponto de dados ao centróide mais próximo usando a distância encontrada na etapa anterior.\n",
    "\n",
    "\n",
    "4. Encontre o novo centróide, calculando a média dos pontos em cada grupo de clusters.\n",
    "\n",
    "\n",
    "5. Repita 2 a 4 para um número fixo de iterações ou até que os centróides não sejam alterados.\n",
    "\n",
    "\n",
    "\n",
    "## Distância euclidiana entre dois pontos no espaço:\n",
    "\n",
    "Se p = (p1, p2) e q = (q1, q2), então a distância é dada por\n",
    "\n",
    "$$ d(p,q) = \\sqrt{ (q_1 - p_1)^2 + (q_2 - p_2)^2 } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo a passo\n",
    "\n",
    "Vamos seguir as etapas acima usando o exemplo abaixo.\n",
    "\n",
    "1. Considere 4 pontos de dados A, B, C, D como abaixo\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/kmeans1.png\" align=\"center\" width=\"15%\">\n",
    "<br>\n",
    "\n",
    "\n",
    "2. Escolha dois centróides AB e CD, calculados como\n",
    "\n",
    "\n",
    "AB = Média de A, B\n",
    "\n",
    "\n",
    "CD = Média de C, D\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/kmeans2.png\" align=\"center\" width=\"17%\">\n",
    "<br>\n",
    "\n",
    "\n",
    "3. Calcule a distância euclidiana ao quadrado entre todos os pontos de dados para os centróides AB, CD. Por exemplo, a distância entre A (2,3) e AB (4,2) pode ser dada por s = (2-4) ² + (3-2) ².\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/kmeans3.png\" align=\"center\" width=\"45%\">\n",
    "<br>\n",
    "\n",
    "4. Se observarmos na figura, a distância destacada entre (A, CD) é 4 e é menor em comparação com (AB, A), que é 5. Como o ponto A está próximo ao CD, podemos mover A para o cluster de CD.\n",
    "\n",
    "\n",
    "5. Existem dois clusters formados até o momento, vamos recalcular os centróides, isto é, B, ACD semelhante à etapa 2.\n",
    "\n",
    "\n",
    "DAC = Média de A, C, D\n",
    "\n",
    "\n",
    "B = B\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/kmeans4.png\" align=\"center\" width=\"17%\">\n",
    "<br>\n",
    "\n",
    "6. Como sabemos que o K-Means é um procedimento iterativo, agora temos que calcular a distância de todos os pontos (A, B, C, D) até novos centróides (B, ACD) semelhantes ao passo 3.\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/kmeans5.png\" align=\"center\" width=\"30%\">\n",
    "<br>\n",
    "\n",
    "7. Na figura acima, podemos ver que os respectivos valores de cluster são mínimos que A está muito longe do cluster B e próximo ao ACD do cluster. Todos os pontos de dados são atribuídos aos clusters (B, ACD) com base em sua distância mínima. O procedimento iterativo termina aqui.\n",
    "\n",
    "\n",
    "8. Para concluir, começamos com dois centróides e terminamos com dois grupos, K = 2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolhendo K\n",
    "\n",
    "## Método Elbow\n",
    "\n",
    "Um método de escolha do valor K é o método do cotovelo. Neste método, executaremos o cluster K-Means para um intervalo de valores K, digamos (K = 1 a 10) e calcularemos a soma do erro quadrático (SSE). O SSE é calculado como a distância média entre os pontos de dados e o centróide do cluster.\n",
    "\n",
    "Em seguida, plote um gráfico de linhas para os valores de SSE para cada K, se o gráfico de linhas parecer um braço, o cotovelo no braço será o valor de K que é o melhor.\n",
    "\n",
    "$$\n",
    "WCSS = \\sum\\limits_{i \\in n} (x_i - c_i)^2 $$\n",
    "\n",
    "onde $c_i$ é o centróide para a observação $x_i$.\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/elbow.png\" align=\"center\" width=\"40%\">\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Score\n",
    "\n",
    "Silhouette refere-se a um método de interpretação e validação de consistência dentro de clusterização. A técnica fornece uma representação gráfica sucinta de quão bem cada objeto foi classificado. \n",
    "\n",
    "O valor da Silhouette é uma medida de quão semelhante um objeto é ao seu próprio cluster (coesão) em comparação com outros clusters (separação). A silhueta varia de -1 a +1, onde um valor alto indica que o objeto é bem correspondido ao seu próprio cluster e mal correspondido aos agrupamentos vizinhos. Se a maioria dos objetos tiver um valor alto, a configuração de cluster será apropriada. Se muitos pontos tiverem um valor baixo ou negativo, a configuração de cluster pode ter muitos ou poucos clusters.\n",
    "\n",
    "A silhueta pode ser calculada com qualquer métrica de distância, como a distância euclidiana ou a distância de Manhattan.\n",
    "\n",
    "$$ \n",
    "s_i = \\frac {b - a}{max(a,b)} $$\n",
    "\n",
    "\n",
    "\n",
    "onde $a$ é a distância média de todas as observações dos pontos do mesmo cluster e $b$ é a distância média mínima de todas as otras observações de todos os outros clusters\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"https://www.researchgate.net/profile/Danielle_Belgrave/publication/304378246/figure/fig2/AS:391240064487430@1470290308933/A-silhouette-plot-used-for-non-hierarchical-clustering-k-means-from-20-with.png\" align=\"center\" width=\"40%\">\n",
    "<br>\n",
    "\n",
    "Para aprofundar [clique aqui](https://en.wikipedia.org/wiki/Silhouette_(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "# Aplicando o KMeans para Dados Sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na célula abaixo, importe make_blobs do módulo datasets do scikit e crie X e y a partir da função `make_blobs` que importamos. Sua documentação está __[aqui](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)__. Vamos começar com 800 samples, 3 features (a dimensionalidade do nosso dataset), 4 grupos e random_state 123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos plotar os dados gerados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na célula abaixo, implemente um k-means com 4 centroides. Lembre-se que o K-Means é um modelo de predição, não transformação, portanto nosso foco será utilizar o `.predict()`. Além disso, estamos falando se aprendizado não-supervisionado, portanto não há labels, treino ou teste, há somente a aplicação do algoritmo no dataset. Crie também um objeto C que recebe o atributo `cluster_centers_` do modelo. <br>\n",
    "Dica: importar classe, instanciar modelo, fitting, predict! \n",
    "\n",
    "Para documentação oficial acesse __[sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora visualizar se nosso algoritmo deu certo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolhendo o número de clusters para o K-Means\n",
    "\n",
    "### Elbow \n",
    "Voilà! Numa análise visual, parece que conseguimos clusterizar nossos dados não categorizados de uma forma viável! Porém, nós demos uma \"roubada\": já sabíamos que existiam 4 clusters, por isso deu certo. Para isso, utilizamos o **Método Elbow**: construímos diversos K-Means com K dentro de um range, e plotamos sua inércia-variância. Numa análise intuitiva, caso escolhemos 1 centroide, a variância será máxima, enquanto que com k = n_samples, a variância será 0. Qual é o ponto de saturação para a variância no nosso K-Means? É justamento esse ponto de inflexão, o \"cotovelo\", que escolheremos como hiperparâmetro do nosso modelo.\n",
    "\n",
    "Abaixo, vamos realizar esse exercício. Vamos criar um loop `for` indo de 1 a 11, no qual dentro dele iremos instanciar e fazer o *fitting* do modelo em X, guardar e printar sua variância acessando o atributo `.inertia` do modelo. Por fim, iremos plotar o resultado e escolher um K:\n",
    "\n",
    "Para um claro exemplo de utilização do WCSS no KMeans e aplicação do método Elbow acesse [K-means Clustering Python Example](https://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellowbricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dois métodos podem ser usados. Geralmente plotamos os dois só pra ter mais certeza sobre a escolha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prós e Contras, Pontos de atenção e etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo do K-Means possui várias vantagens:\n",
    "    \n",
    "- Fácil entendimento\n",
    "- Bem difundido e implementado em várias bibliotecas\n",
    "- Sempre retorna um resultado(também pode ser um contra)\n",
    "\n",
    "Em contrapartida, podemos citar as seguintes desvantagens:\n",
    "- Precisamos definir o K (matamos isso aí usando Elbow e Silhouette)\n",
    "- É afetado por outliers (lembre-se que estamos lidando com médias)\n",
    "- É afetado por efeitos de escala (lembre-se que estamos calculando distâncias, assim como o KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No free lunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem, sim, a regressão linear funciona minimizando a soma dos resíduos quadrados. Mas isso por si só não é o objetivo de uma regressão: o que estamos tentando fazer é desenhar uma linha que sirva como um preditor confiável e imparcial de y com base em x. O teorema de Gauss-Markov nos diz que minimizar o SSE atinge esse objetivo - mas esse teorema se baseia em algumas suposições muito específicas. Se essas suposições forem quebradas, você ainda pode minimizar o SSE, mas pode não fazer nada. Imagine dizer “Você dirige um carro apertando o pedal. O pedal pode ser pressionado independentemente da quantidade de gasolina no tanque. Portanto, mesmo se o tanque estiver vazio, você ainda pode pisar no pedal e dirigir o carro. ”\n",
    "\n",
    "Mas falar é fácil. Vejamos os dados frios e difíceis. Ou, na verdade, dados inventados.\n",
    "<img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/anscombe-1.png\" align='center'>\n",
    "\n",
    "Estes são meus dados inventados favoritos: Quarteto de Anscombe. Criado em 1973 pelo estatístico Francis Anscombe, esta mistura deliciosa ilustra a loucura de confiar cegamente em métodos estatísticos. Cada um dos conjuntos de dados tem o mesmo declive de regressão linear, interceptação, valor p e $ R ^ 2 $ - e ainda à primeira vista podemos ver que apenas um deles, I, é apropriado para regressão linear. Em II ele sugere a forma errada, em III ele é distorcido por um único outlier - e em IV não há nenhuma tendência!\n",
    "\n",
    "Pode-se dizer que \"a regressão linear ainda está funcionando nesses casos, porque está minimizando a soma dos quadrados dos resíduos\". Mas que vitória de Pirro! A regressão linear sempre traçará uma linha, mas se for uma linha sem sentido, quem se importa?\n",
    "\n",
    "Então, agora vemos que só porque uma otimização pode ser realizada não significa que estamos cumprindo nosso objetivo. E vemos que criar dados e visualizá-los é uma boa maneira de inspecionar as suposições de um modelo. Segure essa intuição, vamos precisar dela em um minuto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Há mesmo um padrão?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro ponto de atenção que gostaríamos de levantar é que mesmo uma distribuição uniforme pode gerar clusters no K-means. Na figura abaixo, não necessariamente existem dois grupos diferentes. Há apenas uma massa de dados igualmente distribuída em todos os pontos do espaço. Mesmo assim, o K-Means retorna um resultado pra nós, indicando haver 2 clusters de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/gn1iM.png\" align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Os clusters são esféricos? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O K-means assume que os clusters são esféricos. Ou seja, geometricamente, eles deveriam ser aglomerados, de forma que se parecessem com uma nuvem densa e circular de pontos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando essa suposição é violada, o K-means pode enfrentar alguns problemas. No exemplo abaixo, nós, humanos, percebemos rapidamente a existência de dois agrupamentos nos dados. Contudo, essa condição geométrica viola uma das suposições do K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/non_spherical-1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando utilizamos o K-means para clusterizar esses dados, obtemos a seguinte segmentação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/plot_kmeans-1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Os clusters possuem tamanhos parecidos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos olhar a imagem abaixo.\n",
    "<img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/different_sizes-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualmente, conseguimos notar 3 nuvens de pontos. Para o K-means os clusters deveriam ter um número próximo de elementos/indivíduos. A tendência é que pontos de um cluster acabem migrando para outro. Vejamos a segmentação realizada pelo algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/different_sizes_kmeans-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que apesar do número K ser 3, com esperávamos, a diposição dos pontos em cada cluster pode não fazer sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mínimos Locais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como outros algoritmos de otimização há sempre o risco do K-means cair numa região de mínimos locais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A imagem abaixo mostra vários clusters bem definidos. Porém, o algoritmo K-means não consegue separá-los perfeitamente, nos trazendo a ideia de que ele pode ter caído em uma região de mínimo local. Ou seja, essa solução é subótima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/BILDt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O K-means é, como qualquer outro algoritmo de aprendizado, um processo de otimização. Lembre-se sempre que a otimização irá sempre ocorrer, independentemente do conjunto de dados. Mas nem sempre a solução ótima do problema encontrada pelo algoritmo resolve o problema de negócio. A ideia é sempre inspecionar as métricas e também, se possível, validar os clusters com uma anáçlis exploratória."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
