{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Ensembles: Boosting\n",
    "Continuamos nossas aulas de técnicas de *machine learning* para classificação. Na aula passada falamos sobre métodos de Ensembles Paralelos. Nesta aula, vamos falar sobre métodos de Ensembles sequenciais. Lembrando os tipos de Ensembles:\n",
    "\n",
    "\n",
    "1. ***Parallel Ensemble Methods***: métodos de conjunto paralelo em que os *base learners* são gerados em paralelo (exemplos são os métodos de *Bagging, Stacking, Voting*). A motivação básica dos métodos paralelos é explorar a independência entre os *base learners*, pois o erro pode ser reduzido drasticamente pela média.\n",
    "\n",
    "\n",
    "2. ***Sequential Ensemble Methods***: métodos de conjunto sequencial em que os *base learners* são gerados sequencialmente (exemplos são os métidos de *Boosting*). A motivação básica dos métodos seqüenciais é explorar a dependência entre os *base learners*. O desempenho geral pode ser aprimorado pesando exemplos previamente rotulados incorretamente com maior peso.\n",
    "\n",
    "\n",
    "Dentro desta classificação temos:\n",
    "\n",
    "\n",
    "1. Parallel Ensemble Methods\n",
    "\n",
    "    - Voting Ensemble\n",
    "    - Stacking Ensemble\n",
    "    - Bagging Ensemble\n",
    "    \n",
    "    \n",
    "\n",
    "2. Sequential Ensemble Methods\n",
    "\n",
    "    - Boosting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição de Sequential Boosting Ensemble\n",
    "\n",
    "\n",
    "\n",
    "Boosting traz um conceito popular de combinar vários modelos fracos para obter um modelo forte.\n",
    "O modo como o modelo se baseia em erros sequencialmente é exclusivo para cada algoritmo de boosting.\n",
    "\n",
    "*Boosting* envolve treinar iterativamente os *weak learners*, cada um tentando corrigir o erro cometido pelo modelo anterior. Isso é obtido treinando um modelo fraco em todos os dados de treinamento e construindo um segundo modelo que visa corrigir os erros cometidos pelo primeiro modelo. Em seguida, construímos um terceiro modelo que tenta corrigir os erros cometidos pelo segundo modelo e assim por diante. Os modelos são adicionados iterativamente até o modelo final corrigir todos os erros cometidos por todos os modelos anteriores.\n",
    "\n",
    "Quando os modelos são adicionados em cada estágio, alguns pesos são atribuídos ao modelo, o que está relacionado à precisão do modelo anterior. Após a adição de um classificador fraco, os pesos são reajustados. Os pontos classificados incorretamente recebem pesos mais altos e os pontos classificados corretamente recebem pesos menores. Essa abordagem fará com que o próximo classificador se concentre nos erros cometidos pelo modelo anterior.\n",
    "\n",
    "O aumento reduz o erro de generalização, adotando um modelo de alta e baixa variância e reduzindo a tendência em um nível significativo. Lembre-se, *Bagging* reduz a variação. Semelhante ao *Bagging*, o reforço também permite trabalhar com os modelos de classificação e regressão. Dê uma olhada nos diagramas abaixo para entender intuitivamente como o *Boosting* funciona em cada um dos estágios. O diagrama abaixo mostra os diferentes estágios de um algoritmo de aumento.\n",
    "\n",
    "\n",
    "Vamos entender o diagrama acima. Temos um conjunto de dados D, a primeira coisa que faremos no estágio 0 é treinar um modelo em todo o conjunto de dados. O modelo pode ser uma classificação ou um modelo de regressão. Vamos nomear esse modelo como $M_0$. Vamos assumir que este modelo $M_0$ está tentando ajustar uma função $h_0 (x)$. Então, a função de previsão para este modelo é dada por $y_pred = h_0 (x)$. \n",
    "\n",
    "O modelo 0 foi projetado para ter um alto viés. Geralmente, o reforço é aplicado para um modelo de alto viés e baixa variação. O alto viés em um modelo refere-se basicamente a um alto erro de treinamento. O alto viés surge principalmente devido a algumas suposições incorretas feitas na fase de treinamento.\n",
    "\n",
    "Agora, após a construção do primeiro modelo, obteremos, no estágio 0, o erro de previsão para cada ponto de dados criado pelo modelo $M_0$. Portanto, o erro na previsão para qualquer rótulo de classe é dado por $y-y_pred$. Lembre-se, existem muitas funções de erro por aí - por exemplo, o erro quadrático, o *hinge loss error*, o erro de perda logística etc. Mas, por simplicidade, vamos nos concentrar no erro de diferença simples para este exemplo.\n",
    "\n",
    "Agora que fizemos essas coisas no estágio 0, o que faremos no estágio 1 é o seguinte. Tentarei ajustar um modelo $M_1$ aos erros produzidos pelo modelo no estágio 0. Lembre-se, $M_1$ não está treinando nos rótulos de classe reais. $M_1$ está treinando os erros que obtivemos no final do estágio 0. Digamos que obtemos uma função $h_1 (x)$, que treinou os erros gerados pelo modelo $M_0$. Assim, no final do estágio 1, meu modelo final será realmente a soma ponderada das duas funções de previsão anteriores (como mostrado no diagrama). Atribuiremos os pesos $a_0$ e $a_1$ a $h_0 (x)$ e $h_1 (x)$ respectivamente. Portanto, no final do estágio 1, o modelo se parece com: \n",
    "\n",
    "$$ F_1 (x) = a_0 * h_0 (x) + a_1 * h_1 (x) $$\n",
    "\n",
    "em que a_0 e a_1 são pesos atribuídos às funções de previsão. Lembre-se de que os pesos sempre serão maiores para funções com alto erro de classificação incorreta. Dessa forma, podemos criar o próximo modelo na sequência para focar mais nos erros cometidos pelo modelo anterior.\n",
    "\n",
    "Da mesma forma, o modelo no final do estágio 2 terá a função \n",
    "\n",
    "$$ F_2 (x) = a_0 * h_0 (x) + a_1 * h_1 (x) + a_2 * h_2 (x) $$\n",
    "\n",
    "Assim, no final de todas as etapas, o modelo final que temos é dado pela soma de $a_i * h_x (i)$, onde o valor de i varia de 1 a N. Assim, intuitivamente, estamos na verdade reduzindo o erro de treinamento, o que significa em outras palavras, na verdade estamos reduzindo o viés de um modelo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1000/1*VGSoqefx3Rz5Pws6qpLwOQ@2x.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost, abreviação de Adaptive Boosting, foi criado por Yoav Freund e Robert Schapire. É um dos primeiros algoritmos de sucesso no ramo Boosting do aprendizado de máquina.\n",
    "AdaBoost é um algoritmo popular e excelente para começar ao aprender sobre o mundo do boost.\n",
    "Uma coisa a notar é que AdaBoost realmente funciona com qualquer classificador, como regressão logística, SVMs, etc., desde que sejam modelos fracos. No entanto, AdaBoost é notoriamente discutido no contexto de árvores de decisão de profundidade única chamadas “*stumps*”. \n",
    "Existem algumas razões para isso, mas isso está fora do escopo da aula de hoje.\n",
    "A questão é que o AdaBoost não precisa necessariamente ser usado junto com as árvores de decisão, mas normalmente é feito na prática, devido às vantagens das *stumps* em relação a\n",
    "outros alunos fracos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O  algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*9NjSsLHRSXUwl0cyvKbWGQ.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não se assuste ainda, vamos desvendar cada step desse algoritmo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que o Adaboost é um modelo para classificaçao e regressão. Basicamente, o algoritmo consiste de construir pequenas árvores que só possuem o nó raiz e as quais chamamos de *stumps* combinar o resultado dessas *stumps* em um único resultado. Lembrando que cada *stump* vai ter um peso diferente, baseado no quão precisa elas são."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*DVdys8z6Qobvmb0tFvm2XQ.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma coisa importante a destacar é que o AdaBoost usa especificamente -1 ou 1 como rótulos de predição e produzirá -1 ou 1 durante a previsão. Ele não usa a abordagem binária usual de 0 e 1.\n",
    "Com isso em mente, vamos mergulhar na análise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializaaço dos pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*U16jWJJfhWa5QiQQhoa92g.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No AdaBoost, não usamos diretamente o conjunto de dados, pois é para treinamento. Em vez disso, no início de cada iteração de treinamento, amostramos o conjunto de dados usando pesos para obter os dados de treinamento reais.\n",
    "Portanto, a etapa 1 do AdaBoost é inicializar um peso de 1 / N para cada ponto de dados. Novamente, o peso de um ponto de dados representa a probabilidade de selecioná-lo durante a amostragem.\n",
    "Uma ilustração disso é a seguinte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*2DFPtONH1UpcX1tNAPaPCQ.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos o conjunto de dados acima como nosso exemplo de brinquedo por um tempo.\n",
    "Após a inicialização do peso, entramos no loop de iteração de treinamento na etapa 2.\n",
    "Na primeira iteração (m = 1), cada ponto de dados do conjunto de dados original tem uma chance igual de ser selecionado como uma amostra de treinamento. Portanto, se nosso conjunto de dados for como o acima e tiver 10 pontos de dados, cada um terá uma probabilidade de 0,1 de ser selecionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1000/1*JMm1dNDD6BwkktZXOvYeSQ.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*US8fXwA7fRgcbdO0zOIbXA.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Depois de amostrar o conjunto de dados para obter seu conjunto de treinamento para a iteração atual, um classificador fraco K_m é então treinado usando este conjunto de treinamento. Em nosso caso, K_m é apenas um toco de árvore de decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update dos Pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os *stumps* no AdaBoost aprendem progressivamente com os erros dos *stumps* anteriores, ajustando os pesos do conjunto de dados para acomodar os erros cometidos pelos *stumps* em cada iteração. Os pesos dos dados classificados incorretamente serão aumentados e os pesos dos dados classificados corretamente serão reduzidos. Como resultado, à medida que avançamos em iterações adicionais, os dados de treinamento incluirão principalmente dados que muitas vezes são classificados incorretamente por nossos *stumps*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/732/1*WdnVQz9fm9qqdkQXVCQJyA.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nosso primeiro \"stump* classificou incorretamente as amostras de dados 0,8,5,4, então, no final da iteração 1, as atualizações de peso resultam nessas amostras de dados tendo pesos maiores.\n",
    "Após as atualizações de peso, você pode ver no exemplo abaixo que a nova amostragem do conjunto de dados para obter o conjunto de treino para a iteração 2 resultaria na inclusão de mais amostras de dados classificadas incorretamente (0,8,5,4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1762/1*lJKXXIj9_yBOpB8bDb4NWA.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">OBS: Os updates do exemplo são somente para ilustração do processo, representando os acréscimos e decréscimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Intuitivamente, isso faz sentido: se um toco não é capaz de classificar certos dados corretamente, então você só precisa treinar mais tocos nos dados que são “difíceis de classificar”. Portanto, tomando uma soma ponderada desses tocos, vamos AdaBoost ter um desempenho muito bom.\n",
    "Como resultado, as atualizações de peso são cruciais para que o AdaBoost funcione corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas como realizar o update adequado dos pesos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resposta está nesta seção do algoritmo AdaBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*S9Ohe2OcOpDZKWrAelTFLQ.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa 2e) do algoritmo AdaBoost praticamente resume como as atualizações de peso em determinada iteração m são feitas, elas são calculadas como o produto dos pesos atuais, com o exponencial de (-alpha * y * K_m (x)), onde:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **y** representa o valor real do target\n",
    "- **K_m(x**) é a previsão feita pela *stump* gerada na iteração m\n",
    "- **alpha_m** É a *confiança* que nós temos no poder preditivo da *stump*\n",
    "- **y** and **K_m(x)** podem ser ou -1 ou 1. Se y for 1 and K_m(x) for 1, ou ambos forem -1, isso significa que o registro foi corretamente classificado. Então, o produto y*K_m(x) será igual a 1. Caso contrário, quando a previsão for diferente do valor real, essa multiplicação será negativa.\n",
    "\n",
    "Quando os dados são classificados incorretamente, y * K_m (x) = -1, então a atualização do peso será positiva e os pesos aumentarão exponencialmente para esses dados. Quando os dados são classificados corretamente, y * K_m (x) = 1, ocorre o oposto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No final a combinaçao de pesos e previsões da origem a Cm(x):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/782/1*eCFwnJcHfqwDuYw9xAawig.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A predição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que entendemos matemática e intuitivamente o que alfa representa, tudo o que resta é ver a etapa final do algoritmo AdaBoost, que é fazer previsões gerais sobre novos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*lnQ-KvTdG6VFGiP9Ax_yZw.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso significa apenas que vamos pegar o sinal da soma ponderada.\n",
    "Afinal, AdaBoost prevê apenas {-1,1}, então, desde que a soma ponderada esteja acima de 0, então AdaBoost irá prever 1. O oposto acontece, quando a soma ponderada está abaixo de 0 e AdaBoost irá prever -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiper-Parâmetros:\n",
    "\n",
    "\n",
    "- `base_estimator` \n",
    "    - Ajuda a especificar o tipo de estimador de base, ou seja, o algoritmo de aprendizado de máquina a ser usado como aluno de base.\n",
    "\n",
    "\n",
    "- `n_estimators`\n",
    "    - Ele define o número de estimadores de base. \n",
    "    - O valor padrão é 10, mas você deve manter um valor mais alto para obter melhor desempenho.\n",
    "\n",
    "\n",
    "- `learning_rate`\n",
    "    - Este parâmetro controla a contribuição dos estimadores na combinação final.\n",
    "    - Há uma trade-off entre learning_rate e n_estimators.\n",
    "   \n",
    "- `random_state`\n",
    "    - Um valor inteiro para especificar a divisão de dados aleatórios.\n",
    "    - Um valor definido de random_state sempre produzirá os mesmos resultados se fornecido com os mesmos parâmetros e dados de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Demonstração de Boosting para Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) Primeiramente, criamos um classificador base. Ele vai conter erros, e precisamos corrigí-los"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2) Criamos um segundo classificador, mas que opera em cima dos erros do primeiro (wrong predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3) Continuamos esse loop até chegar na performance desejada. O output final é dado pela Weighted Average dos sub-modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4) Nosso modelo final é uma combinação de todos os outros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmos baseados em boosting:\n",
    " - Adaboost\n",
    " - GBM (Gradient Boosting Machine)\n",
    " - XGBoost (Extreme Gradient Boost)\n",
    " - LightGBM\n",
    " - CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting (GBM)\n",
    "\n",
    "\n",
    "O *Gradient Boosting* ou GBM é outro algoritmo de aprendizado de máquina de conjunto que funciona para problemas de regressão e classificação. O GBM usa a técnica de *boosting*, combinando vários *weak learnenrs* para formar um *strong learner*. Árvores de regressão usadas como *base learner*, cada árvore subsequente em série é construída com base nos erros calculados pela árvore anterior.\n",
    "\n",
    "Usaremos um exemplo simples para entender o algoritmo GBM. Temos que prever a idade de um grupo de pessoas usando os dados abaixo:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Um valor médio é assumido como o valor previsto para todas as observações no conjunto de dados.\n",
    "\n",
    "2. Os erros são calculados usando esta previsão média e os valores reais.\n",
    "\n",
    "3. Um modelo de árvore é criado usando os erros calculados acima como variável de destino. Nosso objetivo é encontrar a melhor divisão para minimizar o erro.\n",
    "\n",
    "4. As previsões desse modelo são combinadas com as previsões 1.\n",
    "\n",
    "5. Este valor calculado acima é a nova previsão.\n",
    "\n",
    "6. Novos erros são calculados usando esse valor previsto e o valor real.\n",
    "\n",
    "7. As etapas 2 a 6 são repetidas até que o número máximo de iterações seja atingido (ou a função de erro não seja alterada).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembra quando estudamos regressão linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das premissas para que fosse um modelo válido e produzisse o melhor estimador possível era de que nossos resíduos tivessem média zero e sua soma também fosse zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*mBStjWVK-yLvPvGYjw-1dA.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora pense nesses resíduos como erros cometidos por nosso modelo preditor. Embora os modelos baseados em árvore (considerando a árvore de decisão como modelos básicos para o nosso aumento de gradiente aqui) não sejam baseados em tais suposições, mas se pensarmos logicamente (não estatisticamente) sobre esta suposição, podemos argumentar que, se formos capazes de ver algum padrão de resíduos em torno de 0, podemos alavancar esse padrão para ajustar um modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No final, nossa previsão é = Modelo + Resíduos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, a intuição que rege o modelo de gradiente boosting é a de entender os padrões de erros gerados e então utilizá-los para fortalecer as previsões fracas dos modelos anteriores. Assim que atingirmos um estágio em que os resíduos não tenham nenhum padrão que possa ser modelado, podemos parar de modelar os resíduos (caso contrário, isso pode levar a um sobreajuste). Algoritmicamente, estamos minimizando nossa função de perda, de forma que a perda do teste atinja seus mínimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar o seguinte conjunto de dados:\n",
    "<img src=\"https://miro.medium.com/max/1172/1*7EPVSm_80fyh-29g8q6yRQ.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Podemos escolher um modelo simples (alto bias) para tentar modelar esses dados. O output desse modelo seria Y_predicted_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Calculamos os erros (resíduos) das previsões desse primeiro modelo. [E1 = Y_real - Y_predicted_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Construímos um segundo modelo, cujas features são as mesmas do primeiro modelo, porém o target é o vetor de resíduos calculado no passo anterior. O output disso chamos de E1_predicted, que será a estimativa dos resíduos do primeiro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Aqui, poderíamos tentar realizar previsões do tipo: Y_predicted_2 = Y_predicted_1 + E1_predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poderíamos parar por aqui, mas podemos continuar adicionando modelos e ajustando erros até que tudo isso vire uma grande combinação de previsões e resíduos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Ajuste outro modelo nos resíduos que ainda restam. ou seja, [E2 = Y - Y_predicted_2] e repetimos as etapas 2 a 5 até que comece a overfitar ou a soma dos resíduos se torne constante. O overfitting pode ser controlado verificando consistentemente a precisão dos dados de validação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trazendo uma intuição visual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*2fGb3jTF85XyHtnpJYA8ug.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1284/1*Ram0yHpCwXWZ23HZUN1QwA.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Observamos que após a 20ª iteração, os resíduos são distribuídos aleatoriamente  em torno de 0 e nossas previsões estão muito próximas dos valores verdadeiros. (as iterações são chamadas de n_estimators na implementação do sklearn). Esse seria um bom ponto para parar ou nosso modelo começará a se ajustar a mais.\n",
    "Vamos ver como fica nosso modelo para a 50ª iteração."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*tNYXUUU23kcoiww26Uh6jw.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que mesmo após a 50ª iteração, os resíduos vs. gráfico x parecem semelhantes ao que vemos na 20ª iteração. Mas o modelo está se tornando mais complexo e as previsões estão se ajustando demais aos dados de treinamento e estão tentando aprender cada dado de treinamento. Portanto, teria sido melhor parar na 20ª iteração."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "O XGBoost (Extreme Gradient Boosting) é uma implementação avançada do algoritmo de *Gradient Boosting*. O XGBoost provou ser um algoritmo ML altamente eficaz, amplamente utilizado em competições de aprendizado de máquina e hackathons. O XGBoost possui alto poder preditivo e é quase 10 vezes mais rápido que as outras técnicas de *Gradient Boosting*. Ele também inclui uma variedade de regularização que reduz o excesso de ajuste e melhora o desempenho geral. Por isso, também é conhecida como técnica de \"reforço regularizado\".\n",
    "\n",
    "Vamos ver como o XGBoost é comparativamente melhor do que outras técnicas:\n",
    "\n",
    "1. Regularização:\n",
    "    - A implementação padrão do GBM não tem regularização como o XGBoost.\n",
    "    - Assim, o XGBoost também ajuda a reduzir o excesso de ajustes.\n",
    "    \n",
    "    \n",
    "2. Processamento paralelo:\n",
    "    - O XGBoost implementa o processamento paralelo e é mais rápido que o GBM.\n",
    "    - O XGBoost também suporta a implementação no Hadoop.\n",
    "\n",
    "\n",
    "3. Alta flexibilidade:\n",
    "    - O XGBoost permite que os usuários definam objetivos de otimização personalizados e critérios de avaliação, adicionando uma nova dimensão ao modelo.\n",
    "    - Tratamento de valores ausentes:\n",
    "    - O XGBoost possui uma rotina integrada para lidar com os valores ausentes.\n",
    "\n",
    "\n",
    "4. Poda de árvores:\n",
    "    - O XGBoost faz divisões até o max_depth especificado e, em seguida, começa a podar a árvore para trás e remove as divisões além das quais não há ganho positivo.\n",
    "\n",
    "\n",
    "5. Validação cruzada incorporada:\n",
    "    - O XGBoost permite que o usuário execute uma validação cruzada a cada iteração do processo de otimização e, portanto, é fácil obter o número ideal exato de iterações de otimização em uma única execução.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM - Light GBM\n",
    "\n",
    "Antes de discutir como o Light GBM funciona, primeiro vamos entender por que precisamos desse algoritmo quando temos tantos outros (como os que vimos acima). O Light GBM supera todos os outros algoritmos quando o conjunto de dados é extremamente grande. Comparado aos outros algoritmos, o Light GBM leva menos tempo para executar em um enorme conjunto de dados.\n",
    "\n",
    "O LightGBM é uma estrutura de aumento de gradiente que usa algoritmos baseados em árvore e segue a abordagem em folha, enquanto outros algoritmos trabalham em um padrão de abordagem em nível. As imagens abaixo ajudarão você a entender a diferença de uma maneira melhor.\n",
    "\n",
    "\n",
    "O crescimento em folha pode causar excesso de ajuste em conjuntos de dados menores, mas isso pode ser evitado usando o parâmetro \"max_depth\" para aprender. Você pode ler mais sobre o Light GBM e sua comparação com o XGB neste artigo [Which algorithm takes the crown: Light GBM vs XGBOOST?](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiper-Parâmetros:\n",
    "\n",
    "\n",
    "- `num_iterations`\n",
    "    - Ele define o número de iterações de reforço a serem executadas.\n",
    "\n",
    "\n",
    "- `num_leaves`\n",
    "    - Este parâmetro é usado para definir o número de folhas a serem formadas em uma árvore.\n",
    "    - No caso do Light GBM, como a divisão ocorre em folhas em vez de em profundidade, num_leaves deve ser menor que 2 ^ (max_depth), caso contrário, pode levar ao sobreajuste.\n",
    "\n",
    "\n",
    "- `min_data_in_leaf`\n",
    "    - Um valor muito pequeno pode causar super ajuste.\n",
    "    - É também um dos parâmetros mais importantes para lidar com o super ajuste.\n",
    "\n",
    "\n",
    "- `max_depth`\n",
    "    - Ele especifica a profundidade ou o nível máximo até o qual uma árvore pode crescer.\n",
    "    - Um valor muito alto para esse parâmetro pode causar super ajuste.\n",
    "    \n",
    "    \n",
    "- `bagging_fraction`\n",
    "    - É usado para especificar a fração de dados a ser usada para cada iteração.\n",
    "    - Este parâmetro é geralmente usado para acelerar o treinamento.\n",
    "\n",
    "\n",
    "- `max_bin`\n",
    "    - Define o número máximo de posições em que os valores dos recursos serão agrupados.\n",
    "    - Um valor menor de max_bin pode economizar muito tempo, pois agrupa os valores dos recursos em compartimentos discretos, o que é computacionalmente barato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost\n",
    "\n",
    "\n",
    "O tratamento de variáveis categóricas é um processo tedioso, especialmente quando você tem um grande número dessas variáveis. Quando suas variáveis categóricas têm muitos rótulos (ou seja, são altamente cardinais), a execução de uma codificação *one-hot-encoding* exponencialmente aumenta a dimensionalidade e torna-se realmente difícil trabalhar com o conjunto de dados.\n",
    "\n",
    "\n",
    "O CatBoost pode lidar automaticamente com variáveis categóricas e não requer amplo pré-processamento de dados, como outros algoritmos de aprendizado de máquina. Para mais informações acesse [CatBoost: A machine learning library to handle categorical (CAT) data automatically](https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/)\n",
    "\n",
    "\n",
    "Hiper-Parâmetros\n",
    "\n",
    "\n",
    "- `loss_function`\n",
    "    - Define a métrica a ser usada para o treinamento.\n",
    "\n",
    "\n",
    "- `iterations`\n",
    "    - O número máximo de árvores que podem ser construídas.\n",
    "    - O número final de árvores pode ser menor ou igual a esse número\n",
    "    \n",
    "    \n",
    "- `learning_rate`\n",
    "    - Define a taxa de aprendizado.\n",
    "    - Usado para reduzir a etapa do gradiente.\n",
    "\n",
    "\n",
    "- `border_count`\n",
    "    - Especifica o número de divisões para recursos numéricos.\n",
    "    - É semelhante ao parâmetro max_bin.\n",
    "    \n",
    "    \n",
    "- `depth`\n",
    "    - Define a profundidade das árvores.\n",
    "\n",
    "\n",
    "- `random_seed`\n",
    "    - Este parâmetro é semelhante ao parâmetro 'random_state' que vimos anteriormente.\n",
    "    - É um valor inteiro para definir a semente aleatória para treinamento.\n",
    " \n",
    "\n",
    "*Ensembles* pode aumentar exponencialmente o desempenho do seu modelo e às vezes pode ser o fator decisivo entre o primeiro e o segundo lugar! Abordamos várias técnicas de *Ensembles* e vimos como essas técnicas são aplicadas em algoritmos de aprendizado de máquina. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparametrization\n",
    "\n",
    "Precisamos tomar alguns cuidados com GBM\n",
    "\n",
    "- Eles têm um grande número de hiperparâmetros - aqueles que podem criar ou quebrar seu modelo.\n",
    "\n",
    "\n",
    "- E ainda por cima, ao contrário da Floresta Aleatória, suas configurações padrão geralmente não são as ideais!\n",
    "\n",
    "\n",
    "Portanto, se você deseja usar os GBMs para modelar seus dados, acredito que seja necessário obter um entendimento de alto nível do que acontece internamente. Você não pode fugir usando-o como uma caixa preta completa.\n",
    "\n",
    " Para entender como parametrizar os algoritmos de **Boosting** acesse [Getting started with Gradient Boosting Machines — using XGBoost and LightGBM parameters](https://towardsdatascience.com/understanding-gradient-boosting-machines-using-xgboost-and-lightgbm-parameters-3af1f9db9700)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prática Guiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, validation_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydataset import data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data('cancer').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('status', axis=1)\n",
    "y=df['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ajustar os hiperparâmetros, há várias coisas que precisamos fazer. Primeiro, precisamos iniciar nosso AdaBoostClassifier com algumas configurações básicas. Em seguida, precisamos criar nossa grade de pesquisa com os hiperparâmetros. Existem dois hiperparâmetros que iremos definir e eles são o número de estimadores (n_estimators) e a taxa de aprendizagem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Faremos uma árvore de decisão apenas para fins de comparação. Primeiro, definiremos os parâmetros para a validação cruzada. Em seguida, usaremos um loop for para executar várias árvores de decisão diferentes. A diferença nas árvores de decisão será sua profundidade. A profundidade é o quão longe a árvore pode ir para purificar a classificação. Quanto mais profundidade, maior a probabilidade de sua árvore de decisão overfitar os dados. A última coisa que faremos é imprimir os resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf=StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass param_name=max_depth, param_range=range(1, 10) as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "train_scores, valid_scores = validation_curve(DecisionTreeClassifier(random_state=123), X, y, \"max_depth\",\n",
    "                                               range (1,10),\n",
    "                                               cv=skf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72      , 0.72      , 0.72      , 0.72      , 0.72      ,\n",
       "        0.72      , 0.72      , 0.71523179, 0.71523179, 0.71523179],\n",
       "       [0.74666667, 0.76666667, 0.75333333, 0.74666667, 0.74666667,\n",
       "        0.74666667, 0.72      , 0.74834437, 0.74172185, 0.74834437],\n",
       "       [0.74666667, 0.79333333, 0.79333333, 0.76      , 0.78      ,\n",
       "        0.78      , 0.74      , 0.78145695, 0.74834437, 0.78145695],\n",
       "       [0.83333333, 0.84      , 0.8       , 0.83333333, 0.79333333,\n",
       "        0.78      , 0.77333333, 0.79470199, 0.8013245 , 0.78145695],\n",
       "       [0.86      , 0.84      , 0.86      , 0.9       , 0.81333333,\n",
       "        0.78      , 0.83333333, 0.83443709, 0.83443709, 0.83443709],\n",
       "       [0.88666667, 0.85333333, 0.92666667, 0.93333333, 0.85333333,\n",
       "        0.86      , 0.87333333, 0.86754967, 0.87417219, 0.89403974],\n",
       "       [0.91333333, 0.88666667, 0.95333333, 0.94666667, 0.9       ,\n",
       "        0.90666667, 0.89333333, 0.91390728, 0.8807947 , 0.9205298 ],\n",
       "       [0.92666667, 0.92      , 0.98666667, 0.96666667, 0.94      ,\n",
       "        0.92      , 0.94      , 0.93377483, 0.92715232, 0.95364238],\n",
       "       [0.93333333, 0.94666667, 0.99333333, 1.        , 0.96666667,\n",
       "        0.94      , 0.96666667, 0.98013245, 0.95364238, 0.98013245]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70588235, 0.70588235, 0.70588235, 0.70588235, 0.70588235,\n",
       "        0.70588235, 0.70588235, 0.75      , 0.75      , 0.75      ],\n",
       "       [0.70588235, 0.52941176, 0.64705882, 0.76470588, 0.70588235,\n",
       "        0.70588235, 0.70588235, 0.625     , 0.75      , 0.6875    ],\n",
       "       [0.70588235, 0.47058824, 0.70588235, 0.70588235, 0.70588235,\n",
       "        0.70588235, 0.76470588, 0.625     , 0.75      , 0.6875    ],\n",
       "       [0.64705882, 0.47058824, 0.70588235, 0.76470588, 0.64705882,\n",
       "        0.70588235, 0.76470588, 0.5625    , 0.875     , 0.6875    ],\n",
       "       [0.58823529, 0.47058824, 0.70588235, 0.82352941, 0.70588235,\n",
       "        0.76470588, 0.58823529, 0.625     , 0.75      , 0.5625    ],\n",
       "       [0.64705882, 0.47058824, 0.70588235, 0.82352941, 0.64705882,\n",
       "        0.70588235, 0.58823529, 0.5625    , 0.8125    , 0.625     ],\n",
       "       [0.64705882, 0.47058824, 0.70588235, 0.70588235, 0.52941176,\n",
       "        0.64705882, 0.64705882, 0.5       , 0.75      , 0.6875    ],\n",
       "       [0.70588235, 0.52941176, 0.70588235, 0.76470588, 0.58823529,\n",
       "        0.64705882, 0.64705882, 0.4375    , 0.6875    , 0.6875    ],\n",
       "       [0.70588235, 0.52941176, 0.70588235, 0.82352941, 0.70588235,\n",
       "        0.70588235, 0.64705882, 0.4375    , 0.75      , 0.625     ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "valid_scores_mean = valid_scores.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(range(1,10), train_scores_mean, alpha=0.5, color='blue', label='train')\n",
    "ax.plot(range(1,10), valid_scores_mean, alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(range(1,10), valid_scores_mean - valid_scores.std(axis=1), \n",
    "                valid_scores_mean + valid_scores.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(range(1,10), valid_scores_mean - 2*valid_scores.std(axis=1), \n",
    "                valid_scores_mean + 2*valid_scores.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.5,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Max_features\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada=AdaBoostClassifier(random_state=13)\n",
    "ada_grid={'n_estimators':[50,500, 1000,2000],\n",
    "             'learning_rate':[.001,0.01,.1]}\n",
    "ada_grid=GridSearchCV(estimator=ada,param_grid=search_grid,\n",
    "                    scoring='accuracy',n_jobs=-1,cv=skf, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   22.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=1, shuffle=True),\n",
       "             estimator=AdaBoostClassifier(random_state=13), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n",
       "                         'n_estimators': [50, 500, 1000, 2000]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'n_estimators': 2000}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(ada_grid.cv_results_).sort_values(by='rank_test_score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7363970588235295"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.881378</td>\n",
       "      <td>0.050930</td>\n",
       "      <td>0.454367</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 2000}</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.736397</td>\n",
       "      <td>0.104506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.986507</td>\n",
       "      <td>0.019699</td>\n",
       "      <td>0.113775</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 500}</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.134309</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.911132</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.221859</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 1000}</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.730882</td>\n",
       "      <td>0.092364</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.100461</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.013566</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 50}</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.095585</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.972429</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.112789</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 500}</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.724632</td>\n",
       "      <td>0.103966</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095650</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 50}</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.719118</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.714099</td>\n",
       "      <td>0.126137</td>\n",
       "      <td>0.337135</td>\n",
       "      <td>0.059719</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 2000}</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.711765</td>\n",
       "      <td>0.113914</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.972197</td>\n",
       "      <td>0.026887</td>\n",
       "      <td>0.241909</td>\n",
       "      <td>0.010134</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 1000}</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.711397</td>\n",
       "      <td>0.135195</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.767167</td>\n",
       "      <td>0.059106</td>\n",
       "      <td>0.435757</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 2000}</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.706985</td>\n",
       "      <td>0.085750</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.933389</td>\n",
       "      <td>0.034175</td>\n",
       "      <td>0.092933</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.001</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 500}</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.701103</td>\n",
       "      <td>0.062132</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.096112</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_estimators': 50}</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.701103</td>\n",
       "      <td>0.062132</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.814617</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>0.214134</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.001, 'n_estimators': 1000}</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.688603</td>\n",
       "      <td>0.063594</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7        3.881378      0.050930         0.454367        0.009783   \n",
       "9        0.986507      0.019699         0.113775        0.003254   \n",
       "6        1.911132      0.010392         0.221859        0.005514   \n",
       "8        0.100461      0.000834         0.013566        0.000342   \n",
       "5        0.972429      0.027690         0.112789        0.003476   \n",
       "0        0.095650      0.002531         0.012156        0.001543   \n",
       "11       3.714099      0.126137         0.337135        0.059719   \n",
       "10       1.972197      0.026887         0.241909        0.010134   \n",
       "3        3.767167      0.059106         0.435757        0.012740   \n",
       "1        0.933389      0.034175         0.092933        0.001820   \n",
       "4        0.096112      0.004594         0.013214        0.001383   \n",
       "2        1.814617      0.033312         0.214134        0.006958   \n",
       "\n",
       "   param_learning_rate param_n_estimators  \\\n",
       "7                 0.01               2000   \n",
       "9                  0.1                500   \n",
       "6                 0.01               1000   \n",
       "8                  0.1                 50   \n",
       "5                 0.01                500   \n",
       "0                0.001                 50   \n",
       "11                 0.1               2000   \n",
       "10                 0.1               1000   \n",
       "3                0.001               2000   \n",
       "1                0.001                500   \n",
       "4                 0.01                 50   \n",
       "2                0.001               1000   \n",
       "\n",
       "                                            params  split0_test_score  \\\n",
       "7    {'learning_rate': 0.01, 'n_estimators': 2000}           0.823529   \n",
       "9      {'learning_rate': 0.1, 'n_estimators': 500}           0.882353   \n",
       "6    {'learning_rate': 0.01, 'n_estimators': 1000}           0.823529   \n",
       "8       {'learning_rate': 0.1, 'n_estimators': 50}           0.823529   \n",
       "5     {'learning_rate': 0.01, 'n_estimators': 500}           0.823529   \n",
       "0     {'learning_rate': 0.001, 'n_estimators': 50}           0.705882   \n",
       "11    {'learning_rate': 0.1, 'n_estimators': 2000}           0.764706   \n",
       "10    {'learning_rate': 0.1, 'n_estimators': 1000}           0.882353   \n",
       "3   {'learning_rate': 0.001, 'n_estimators': 2000}           0.764706   \n",
       "1    {'learning_rate': 0.001, 'n_estimators': 500}           0.764706   \n",
       "4      {'learning_rate': 0.01, 'n_estimators': 50}           0.764706   \n",
       "2   {'learning_rate': 0.001, 'n_estimators': 1000}           0.764706   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "7            0.529412           0.705882           0.764706   \n",
       "9            0.529412           0.705882           0.823529   \n",
       "6            0.529412           0.705882           0.764706   \n",
       "8            0.529412           0.705882           0.823529   \n",
       "5            0.529412           0.705882           0.823529   \n",
       "0            0.705882           0.705882           0.705882   \n",
       "11           0.529412           0.588235           0.823529   \n",
       "10           0.529412           0.588235           0.823529   \n",
       "3            0.529412           0.705882           0.764706   \n",
       "1            0.529412           0.705882           0.705882   \n",
       "4            0.529412           0.705882           0.705882   \n",
       "2            0.529412           0.705882           0.705882   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "7            0.882353           0.705882           0.764706   \n",
       "9            0.941176           0.705882           0.764706   \n",
       "6            0.823529           0.705882           0.705882   \n",
       "8            0.764706           0.647059           0.705882   \n",
       "5            0.823529           0.647059           0.705882   \n",
       "0            0.705882           0.705882           0.705882   \n",
       "11           0.882353           0.705882           0.823529   \n",
       "10           0.882353           0.647059           0.823529   \n",
       "3            0.705882           0.705882           0.705882   \n",
       "1            0.705882           0.705882           0.705882   \n",
       "4            0.705882           0.705882           0.705882   \n",
       "2            0.705882           0.705882           0.705882   \n",
       "\n",
       "    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "7              0.6250             0.8750             0.6875         0.736397   \n",
       "9              0.5000             0.8125             0.6875         0.735294   \n",
       "6              0.6875             0.8750             0.6875         0.730882   \n",
       "8              0.6875             0.8750             0.6875         0.725000   \n",
       "5              0.6250             0.8750             0.6875         0.724632   \n",
       "0              0.7500             0.7500             0.7500         0.719118   \n",
       "11             0.5625             0.7500             0.6875         0.711765   \n",
       "10             0.5000             0.7500             0.6875         0.711397   \n",
       "3              0.6250             0.8750             0.6875         0.706985   \n",
       "1              0.7500             0.7500             0.6875         0.701103   \n",
       "4              0.7500             0.7500             0.6875         0.701103   \n",
       "2              0.6250             0.7500             0.6875         0.688603   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "7         0.104506                1  \n",
       "9         0.134309                2  \n",
       "6         0.092364                3  \n",
       "8         0.095585                4  \n",
       "5         0.103966                5  \n",
       "0         0.020217                6  \n",
       "11        0.113914                7  \n",
       "10        0.135195                8  \n",
       "3         0.085750                9  \n",
       "1         0.062132               10  \n",
       "4         0.062132               10  \n",
       "2         0.063594               12  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_gbm = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 5),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 5),\n",
    "    \"max_depth\":[3,5,8, 12],\n",
    "    \"n_estimators\":[10, 30, 70, 100, 500, 1000, 2000]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2800 candidates, totalling 28000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:   55.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1836 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2498 tasks      | elapsed: 19.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3436 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4302 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5396 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6446 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7668 tasks      | elapsed: 22.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9158 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10508 tasks      | elapsed: 24.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11958 tasks      | elapsed: 25.4min\n",
      "[Parallel(n_jobs=-1)]: Done 13580 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-1)]: Done 15230 tasks      | elapsed: 27.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16980 tasks      | elapsed: 28.7min\n",
      "[Parallel(n_jobs=-1)]: Done 18830 tasks      | elapsed: 29.9min\n",
      "[Parallel(n_jobs=-1)]: Done 20780 tasks      | elapsed: 31.2min\n",
      "[Parallel(n_jobs=-1)]: Done 22902 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=-1)]: Done 25052 tasks      | elapsed: 34.1min\n",
      "[Parallel(n_jobs=-1)]: Done 27302 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=-1)]: Done 28000 out of 28000 | elapsed: 35.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=1, shuffle=True),\n",
       "             estimator=GradientBoostingClassifier(random_state=1), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
       "                         'max_depth': [3, 5, 8, 12],\n",
       "                         'min_samples_leaf': array([0.1, 0.2, 0.3, 0.4, 0.5]),\n",
       "                         'min_samples_split': array([0.1, 0.2, 0.3, 0.4, 0.5]),\n",
       "                         'n_estimators': [10, 30, 70, 100, 500, 1000, 2000]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = GradientBoostingClassifier(random_state=1)\n",
    "gbm_grid = GridSearchCV(gbm, parameters_gbm, cv=skf, n_jobs=-1, verbose=1)\n",
    "gbm_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7551470588235294"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
