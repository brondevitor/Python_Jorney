{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulariza√ß√£o em Machine Learning\n",
    "\n",
    "Um dos principais aspectos do treinamento do seu modelo de aprendizado de m√°quina √© evitar o overfitting, pois neste caso o modelo ter√° uma baixa precis√£o. Isso acontece porque o seu modelo dificilmente ir√° conseguir capturar o ru√≠do em seu conjunto de dados de treinamento. Por ru√≠do, queremos dizer os pontos de dados que realmente n√£o representam as propriedades reais de seus dados, mas de chance aleat√≥ria. Aprender esses pontos de dados torna o seu modelo mais flex√≠vel, sob o risco de overfitting.\n",
    "\n",
    "O conceito de balanceamento de vi√©s e vari√¢ncia √© √∫til para entender o fen√¥meno do overfitting e varia√ß√£o de balanceamento, para controlar erros do Machine Leanrning.\n",
    "\n",
    "## Formas de Regulariza√ß√£o\n",
    "\n",
    "Vamos abordar aqui duas formas de se fazer a regulariza√ß√£o:\n",
    "\n",
    "I. Aumentar o conjunto de dados (n√∫mero de observa√ß√µes) \\\n",
    "II. Penaliza√ß√£o dos otimizadores\n",
    "\n",
    "## Aumento do conjunto de dados\n",
    "\n",
    "O Overfitting pode ser controlado aumentando o tamanho do conjunto de dados de treinamento. Aumentar o tamanho do conjunto de dados para evitar ajustes excessivos se refere ao aumento do n√∫mero de observa√ß√µes (ou linhas) e n√£o do n√∫mero de recursos (ou colunas). A adi√ß√£o de colunas pode levar ao aumento da complexidade do problema e, portanto, pode resultar em um pior desempenho. A seguir veremos um exemplo de como aumentar um conjunto de dados para Regularizar um modelo\n",
    "\n",
    "## Penaliza√ß√£o de otimizadores\n",
    "\n",
    "Regulariza√ß√£o √© uma t√©cnica que restringe/regulariza/reduz as estimativas dos coeficientes, diminuiindo seus valores, que ficam portanto mais pr√≥ximos de zero. Em outras palavras, esta t√©cnica desencoraja a aprendizagem de um modelo mais complexo ou flex√≠vel, de modo a evitar o risco de overfitting.\n",
    "\n",
    "Uma rela√ß√£o simples de regress√£o linear se parece com a f√≥rmula abaixo, onde $y$ representa a rela√ß√£o aprendida e ùúÉ representa as estimativas de coeficiente para diferentes vari√°veis ou preditores (x).\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "$$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + ... + \\theta_nx_n$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "O procedimento de ajuste envolve uma fun√ß√£o de perda, conhecida como soma residual de quadrados ou RSS, sendo que os coeficientes s√£o escolhidos, de forma que minimizem essa fun√ß√£o de perda.\n",
    "\n",
    "A regulariza√ß√£o ajustar√° os coeficientes com base nos seus dados de treinamento. Se houver ru√≠do nos dados de treinamento, os coeficientes estimados n√£o ser√£o generalizados nos dados futuros. √â aqui que a regulariza√ß√£o entra e encolhe ou regulariza essas estimativas aprendidas para zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "# Tipos de Regulariza√ß√£o por Penaliza√ß√£o dos Otimizadores\n",
    "\n",
    "1. Ridge\n",
    "2. Lasso\n",
    "3. ElasticNet\n",
    "\n",
    "## 1. Ridge Regression\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "$$Ridge : \\lambda \\sum_{i=1}^{n} \\theta_i^2$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "A f√≥rmula acima mostra o coeficiente de Ridge que modifica a regress√£o pela adi√ß√£o da quantidade de contra√ß√£o. Agora, os coeficientes s√£o estimados minimizando essa fun√ß√£o, e o Œª √© o par√¢metro de sintonia que decide quanto queremos penalizar a flexibilidade do nosso modelo. O aumento na flexibilidade de um modelo √© representado pelo aumento de seus coeficientes e, se quisermos minimizar a fun√ß√£o acima, esses coeficientes precisam ser pequenos. √â assim que a t√©cnica de regress√£o de Ridge impede que os coeficientes subam demais. Al√©m disso, observe que encolhemos a associa√ß√£o estimada de cada vari√°vel com a resposta, exceto o intercepto $\\theta_0$. \n",
    "\n",
    "Quando Œª = 0, o termo de penalidade n√£o tem efeito, e as estimativas produzidas por regress√£o de ridge ser√£o iguais a m√≠nimos quadrados. No entanto, como Œª ‚Üí ‚àû, o impacto da penalidade de contra√ß√£o aumenta, e as estimativas coeficientes da regress√£o se aproximam de zero. Como pode ser visto, selecionar um bom valor de Œª √© cr√≠tico. A valida√ß√£o cruzada √© √∫til para esse prop√≥sito. As estimativas dos coeficientes produzidas por este m√©todo s√£o tamb√©m conhecidas como norma L2.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*hAGhQehrqAmT1pvz3q4t8Q.png\" width=500>\n",
    "<br>\n",
    "Podemos ver esse problema da seguinte forma:\n",
    "<br>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*sC4KLMHU0j_1gR3VmlgGtg.png\" width=300>\n",
    "\n",
    "Os coeficientes que s√£o produzidos pelo m√©todo dos m√≠nimos quadrados padr√£o s√£o escala equivariante, ou seja, se multiplicarmos cada entrada por c, ent√£o os coeficientes correspondentes s√£o escalonados por um fator de 1 / c. Portanto, independentemente de como o preditor √© escalado, a multiplica√ß√£o de preditor e coeficiente permanece a mesma. No entanto, esse n√£o √© o caso da regress√£o de Ridge e, portanto, precisamos padronizar os preditores ou trazer os preditores para a mesma escala antes de executar a regress√£o de Ridge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U√©, mas n√£o era para a regress√£o regularizada generalizar melhor? Em tese, ele generaliza melhor, mas sempre temos que testar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Lasso Regression\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "$$Lasso : \\lambda \\sum_{i=1}^{n} |\\theta_i| $$\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*P5Lq5mAi4WAch7oIeiS3WA.png\" width=400>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*JH9eAS2I9mwOpuFLg-gD6g.png\" width=300>\n",
    "\n",
    "Lasso √© outra varia√ß√£o de regulariza√ß√£o, em que a fun√ß√£o acima √© minimizada. √â claro que essa varia√ß√£o difere da regress√£o de Ridge apenas em penalizar os altos coeficientes. Ele usa | $\\theta_i$ | (m√≥dulo) em vez de quadrados de $\\theta_i$, como sua penalidade. Nas estat√≠sticas, isso √© conhecido como a norma L1.\n",
    "\n",
    "Vamos dar uma olhada nos m√©todos acima com uma perspectiva diferente: a regress√£o de Ridge pode ser pensada como resolvendo uma equa√ß√£o, onde a soma dos quadrados dos coeficientes √© menor ou igual a s. E o Lasso pode ser pensado como uma equa√ß√£o onde a soma do m√≥dulo de coeficientes √© menor ou igual a s. Aqui, s √© uma constante que existe para cada valor do fator de encolhimento Œª. Essas equa√ß√µes tamb√©m s√£o chamadas de fun√ß√µes de restri√ß√£o.\n",
    "\n",
    "Considere os seus dois par√¢metros em um determinado problema. Ent√£o, de acordo com a formula√ß√£o acima, a regress√£o de Ridge √© expressa por $\\theta_1^2 +  \\theta_2^2 ‚â§ c$. Isto implica que os coeficientes de regress√£o de Ridge t√™m a menor RSS (fun√ß√£o de perda) para todos os pontos que se encontram dentro da circunfer√™ncia dada por $\\theta_1^2 +  \\theta_2^2 ‚â§ c$.\n",
    "\n",
    "Da mesma forma, para Lasso, a equa√ß√£o se torna $ |\\theta_1| +  |\\theta_2| ‚â§ t $. Isto implica que os coeficientes de Lasso possuem o menor RSS (fun√ß√£o de perda) para todos os pontos que est√£o dentro do diamante dado por $ |\\theta_1| +  |\\theta_2| ‚â§ t $.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*Jd03Hyt2bpEv1r7UijLlpg.png\" style=\"height:350px\">\n",
    "<br>\n",
    "\n",
    "A imagem acima mostra as fun√ß√µes de restri√ß√£o (√°reas verdes), para Lasso (esquerda) e regress√£o de Ridge (direita), juntamente com contornos para RSS (elipse vermelha). Pontos na elipse compartilham o valor do RSS. Para um valor muito grande de s, as regi√µes verdes conter√£o o centro da elipse, fazendo estimativas de coeficiente de ambas as t√©cnicas de regress√£o, iguais √†s estimativas de m√≠nimos quadrados. Mas, este n√£o √© o caso na imagem acima. Nesse caso, as estimativas do coeficiente de regress√£o de Lasso e Ridge s√£o fornecidas pelo primeiro ponto em que uma elipse entra em contato com a regi√£o de restri√ß√£o. Como a regress√£o de Ridge possui uma restri√ß√£o circular sem pontos agudos, essa interse√ß√£o geralmente n√£o ocorre em um eixo e, portanto, as estimativas do coeficiente de regress√£o de Ridge ser√£o exclusivamente diferentes de zero. No entanto, a restri√ß√£o de Lasso tem cantos em cada um dos eixos e, portanto, a elipse geralmente cruza a regi√£o de restri√ß√£o em um eixo. Quando isso ocorre, um dos coeficientes ser√° igual a zero. Em dimens√µes mais altas (onde os par√¢metros s√£o muito maiores que 2), muitas das estimativas de coeficiente podem ser iguais a zero simultaneamente.\n",
    "\n",
    "Isso esclarece a desvantagem √≥bvia da regress√£o de Ridge, que √© a interpretabilidade do modelo. Reduzir√° os coeficientes dos preditores menos importantes, muito pr√≥ximos de zero. Mas isso nunca os tornar√° exatamente zero. Em outras palavras, o modelo final incluir√° todos os preditores. No entanto, no caso do Lasso, a penalidade de L1 tem o efeito de for√ßar algumas das estimativas de coeficiente a serem exatamente iguais a zero quando o par√¢metro de ajuste Œª √© suficientemente grande. Portanto, o m√©todo do Lasso tamb√©m realiza sele√ß√£o de vari√°veis ‚Äã‚Äãe √© dito que produz modelos escassos.\n",
    "\n",
    "## 3. ElasticNet Regression (L1 + L2)\n",
    "\n",
    "A regress√£o ElasticNet combina o poder da regress√£o Ridge e Lasso em um algoritmo. O que isso significa √© que, com a ElasticNet, o algoritmo pode remover vari√°veis fracas como no Lasso ou reduzi-las a quase zero como no Ridge. Todos esses algoritmos s√£o exemplos de Regulariza√ß√£o.\n",
    "\n",
    "Para ler mais acesse o artigo [Elastic Net Regression in Python](https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Trade-off entre vi√©s e vari√¢ncia\n",
    "\n",
    "O modelo de regress√£o linear simples, no qual voc√™ pretende predizer n observa√ß√µes da vari√°vel de resposta, Y, com uma combina√ß√£o linear de m vari√°veis preditoras, X e um termo de erro normalmente distribu√≠do com varia√ß√£o œÉ2, √© definido por uma equa√ß√£o linear. Como n√£o conhecemos os par√¢metros verdadeiros, temos que estimar esses par√¢metros a partir da amostra. Na abordagem de m√≠nimos quadrados ordin√°rios (OLS), estimamos-os como Œ≤ estimados de tal maneira que a soma dos quadrados dos res√≠duos √© a menor poss√≠vel. \n",
    "\n",
    "\n",
    "\n",
    "Para ler mais acesse o artigo [Regularization: Ridge, Lasso and Elastic Net](https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net)\n",
    "\n",
    "## O que a Regulariza√ß√£o faz exatamente?\n",
    "\n",
    "Um modelo padr√£o de m√≠nimos quadrados tende a apresentar alguma varia√ß√£o, ou seja, esse modelo n√£o generaliza bem para um conjunto de dados diferente dos dados de treinamento. **A regulariza√ß√£o reduz significativamente a vari√¢ncia do modelo, sem aumento substancial de seu vi√©s.** Portanto, o par√¢metro de ajuste Œª, usado nas t√©cnicas de regulariza√ß√£o descritas acima, controla o impacto na varia√ß√£o. √Ä medida que o valor de Œª aumenta, reduz o valor dos coeficientes e, assim, reduz a varia√ß√£o. At√© certo ponto, esse aumento em Œª √© ben√©fico, pois reduz apenas a varia√ß√£o (evitando, portanto, o ajuste excessivo), sem perder propriedades importantes nos dados. Mas, ap√≥s um certo valor, o modelo come√ßa a perder propriedades importantes, dando origem a um vi√©s no modelo e, portanto, a um ajuste insuficiente. Portanto, o valor de Œª deve ser cuidadosamente selecionado.\n",
    "\n",
    "√â tudo o que voc√™ precisa para come√ßar a regularizar. √â uma t√©cnica √∫til que pode ajudar a melhorar a precis√£o dos seus modelos de regress√£o. Uma biblioteca popular para implementar esses algoritmos √© o Scikit-Learn. Ele tem uma API maravilhosa que pode fazer seu modelo funcionar com apenas algumas linhas de c√≥digo em python.\n",
    "\n",
    "Artigo original [Regularization in Machine Learning](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Um pouco de Pr√°tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O desemprego √© uma grande preocupa√ß√£o socioecon√¥mica e pol√≠tica para qualquer pa√≠s e, portanto, gerenci√°-lo √© a principal tarefa de qualquer governo. Nesta pr√°tica, tentaremos construir algoritmos de regress√£o para prever o desemprego em uma economia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir o Passo a Passo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carregar as bibliotecas, m√≥dulos, classes e fun√ß√µes\n",
    "- Carregar os dados\n",
    "- Definir os preditores e a vari√°vel resposta\n",
    "- Criar os conjuntos de treino e teste\n",
    "- Construir os modelos de regress√£o, prever e avaliar os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sugest√£o de Pr√°tica: Realizar valida√ß√£o cruzada para identificar melhor alfa. Testar Lasso, Ridge e ElasticNet. \n",
    "O ElasticNet possui um par√¢metro de peso entre Lasso e Ridge. √â poss√≠vel ajustar tamb√©m esse valor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nteract": {
   "version": "nteract-on-jupyter@1.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
